{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 25 - Preprocessing and Feature Engineering\n",
    "\n",
    "Check the official documentation site for `pyspark.ml.feature` for the most recent version of Spark (or the one that you have installed).\n",
    "\n",
    "http://spark.apache.org/docs/2.4.0/api/python/pyspark.ml.html#module-pyspark.ml.feature\n",
    "\n",
    "## Loading the Input Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"chapter25\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "                                .option(\"inferSchema\", \"true\")\\\n",
    "                                .option(\"path\", \"../data/retail-data/by-day/*.csv\")\\\n",
    "                                .load().coalesce(5).where(\"Description IS NOT NULL\")\n",
    "sales.cache()  # make it persistent, since we will be using it later\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF = spark.read.parquet(\"../data/simple-ml-integers\")\n",
    "fakeIntDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red|good|    45| 38.97187133755819|\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleDF = spark.read.json(\"../data/simple-ml\")\n",
    "simpleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaleDF = spark.read.parquet(\"../data/simple-ml-scaling\")\n",
    "scaleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers and Estimators\n",
    "\n",
    "**Transformers** do not depend on the input data, but they work for any dataset. An example would be a tokenizer. Transformers have a `transform()` method.\n",
    "\n",
    "**Estimators** can be used for preprocessing and depend on the input data, thus they have a `fit()` method. After the method `tranform()` would be called. An example is a Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------------------+\n",
      "|         Description|Tokenizer_4c03a9c1298975c4937e__output|\n",
      "+--------------------+--------------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|                  [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |                  [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|                  [12, message, car...|\n",
      "|BLUE HARMONICA IN...|                  [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|                  [gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|                  [skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|                  [feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|                  [camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|                  [white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|                  [english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|                  [hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|                  [scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|                  [rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|                  [gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|                  [storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|                  [set, of, 4, knic...|\n",
      "|      POPCORN HOLDER|                     [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|                  [grow, a, flytrap...|\n",
      "|AIRLINE BAG VINTA...|                  [airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|                  [airline, bag, vi...|\n",
      "+--------------------+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer(inputCol=\"Description\")\n",
    "tkn.transform(sales.select(\"Description\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------------------+\n",
      "| id|      features|StandardScaler_466c92924d19a3a9964e__output|\n",
      "+---+--------------+-------------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|                       [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|                       [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|                       [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|                       [3.58568582800318...|\n",
      "+---+--------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "ss = StandardScaler(inputCol=\"features\")\n",
    "ss.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level transformers\n",
    "\n",
    "As a general principle, you should aim to:\n",
    "\n",
    "* Always use high-level transformers, avoid re-implemeting since the implementation has been optimized for Spark.\n",
    "* Use the highest level transformer available.\n",
    "\n",
    "### RFormula\n",
    "\n",
    "This has been already demoed in [Chapter 24](https://github.com/drublackberry/Spark-The-Definitive-Guide/blob/master/notebook/Chapter24.ipynb) but it is reshown here for consistency. To understand it better some help can be found [here](https://spark.apache.org/docs/latest/ml-features.html#rformula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(color='green', lab='good', value1=1, value2=14.386294994851129, features=SparseVector(10, {1: 1.0, 2: 1.0, 3: 14.3863, 5: 1.0, 8: 14.3863}), label=1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "foo = supervised.fit(simpleDF).transform(simpleDF)\n",
    "foo.show()\n",
    "foo.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL transformer\n",
    "Just remember to use the keyword `__THIS__` to refer to the table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "|           34|       6|   14768.0|\n",
      "|         1542|      30|   13094.0|\n",
      "|          854|     117|   17884.0|\n",
      "|           97|      12|   16596.0|\n",
      "|          290|      98|   13607.0|\n",
      "|          541|      27|   14285.0|\n",
      "|          244|      31|   16561.0|\n",
      "|          491|     152|   13956.0|\n",
      "|          204|      76|   13533.0|\n",
      "|          493|      64|   16629.0|\n",
      "|          159|      38|   17267.0|\n",
      "|         1140|      30|   13918.0|\n",
      "|           55|      28|   18114.0|\n",
      "|           88|       7|   14473.0|\n",
      "|          150|      16|   14024.0|\n",
      "|          206|      23|   12493.0|\n",
      "|          138|      18|   15776.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransform = SQLTransformer()\\\n",
    "                .setStatement(\"\"\"SELECT sum(Quantity), count(*), CustomerID FROM __THIS__ GROUP BY CustomerID\"\"\")\n",
    "basicTransform.transform(sales).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler\n",
    "It assembles the values into a `Vector` of features that can be fed to an Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_47b4a41a1854e456259e__output|\n",
      "+----+----+----+--------------------------------------------+\n",
      "|   1|   2|   3|                               [1.0,2.0,3.0]|\n",
      "|   4|   5|   6|                               [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                               [7.0,8.0,9.0]|\n",
      "+----+----+----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Continuous Features\n",
    "\n",
    "### Bucketing\n",
    "\n",
    "#### Bucketizer\n",
    "Splits according to defined buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "| 0.0|\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 4.0|\n",
      "| 5.0|\n",
      "| 6.0|\n",
      "| 7.0|\n",
      "| 8.0|\n",
      "| 9.0|\n",
      "|10.0|\n",
      "|11.0|\n",
      "|12.0|\n",
      "|13.0|\n",
      "|14.0|\n",
      "|15.0|\n",
      "|16.0|\n",
      "|17.0|\n",
      "|18.0|\n",
      "|19.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "contDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------+\n",
      "|  id|Bucketizer_4d76b5eea808998251f6__output|\n",
      "+----+---------------------------------------+\n",
      "| 0.0|                                    0.0|\n",
      "| 1.0|                                    0.0|\n",
      "| 2.0|                                    0.0|\n",
      "| 3.0|                                    0.0|\n",
      "| 4.0|                                    0.0|\n",
      "| 5.0|                                    1.0|\n",
      "| 6.0|                                    1.0|\n",
      "| 7.0|                                    1.0|\n",
      "| 8.0|                                    1.0|\n",
      "| 9.0|                                    1.0|\n",
      "|10.0|                                    2.0|\n",
      "|11.0|                                    2.0|\n",
      "|12.0|                                    2.0|\n",
      "|13.0|                                    2.0|\n",
      "|14.0|                                    2.0|\n",
      "|15.0|                                    2.0|\n",
      "|16.0|                                    2.0|\n",
      "|17.0|                                    2.0|\n",
      "|18.0|                                    2.0|\n",
      "|19.0|                                    2.0|\n",
      "+----+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucket_borders = [-1., 5., 10., 250., 600.]\n",
    "bucketer = Bucketizer().setSplits(bucket_borders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QuantileDiscretizer\n",
    "\n",
    "Splits in buckets based on quantiles, the input given is the number of quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------------+\n",
      "|  id|QuantileDiscretizer_4428a80c01321694f258__output|\n",
      "+----+------------------------------------------------+\n",
      "| 0.0|                                             0.0|\n",
      "| 1.0|                                             0.0|\n",
      "| 2.0|                                             0.0|\n",
      "| 3.0|                                             1.0|\n",
      "| 4.0|                                             1.0|\n",
      "| 5.0|                                             1.0|\n",
      "| 6.0|                                             1.0|\n",
      "| 7.0|                                             2.0|\n",
      "| 8.0|                                             2.0|\n",
      "| 9.0|                                             2.0|\n",
      "|10.0|                                             2.0|\n",
      "|11.0|                                             2.0|\n",
      "|12.0|                                             3.0|\n",
      "|13.0|                                             3.0|\n",
      "|14.0|                                             3.0|\n",
      "|15.0|                                             4.0|\n",
      "|16.0|                                             4.0|\n",
      "|17.0|                                             4.0|\n",
      "|18.0|                                             4.0|\n",
      "|19.0|                                             4.0|\n",
      "+----+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bucketing based on Quantiles\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "bucketer = QuantileDiscretizer(numBuckets=5, inputCol=\"id\")\n",
    "fitted_bucketer = bucketer.fit(contDF)\n",
    "fitted_bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: UnbalancedQuantileDiscretizer\n",
    "\n",
    "It is a quantile discretizer whose inputs are a quantile array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------+\n",
      "|  id|Bucketizer_416c80ae20dc2ea29e7c__output|\n",
      "+----+---------------------------------------+\n",
      "| 0.0|                                    0.0|\n",
      "| 1.0|                                    0.0|\n",
      "| 2.0|                                    0.0|\n",
      "| 3.0|                                    0.0|\n",
      "| 4.0|                                    0.0|\n",
      "| 5.0|                                    0.0|\n",
      "| 6.0|                                    0.0|\n",
      "| 7.0|                                    0.0|\n",
      "| 8.0|                                    0.0|\n",
      "| 9.0|                                    1.0|\n",
      "|10.0|                                    1.0|\n",
      "|11.0|                                    1.0|\n",
      "|12.0|                                    1.0|\n",
      "|13.0|                                    1.0|\n",
      "|14.0|                                    2.0|\n",
      "|15.0|                                    2.0|\n",
      "|16.0|                                    2.0|\n",
      "|17.0|                                    2.0|\n",
      "|18.0|                                    2.0|\n",
      "|19.0|                                    3.0|\n",
      "+----+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_quantiles = [0.5, 0.75, 0.99]\n",
    "\n",
    "class UnbalancedQuantileDiscretizer:\n",
    "    \n",
    "    def __init__(self, quantiles):\n",
    "        self.quantiles = quantiles\n",
    "        self.quantile_values = []\n",
    "        self.inputCol = ''\n",
    "    \n",
    "    def fit(self, df_in, inputCol, relativeError=1e-2):\n",
    "        quantile_values = df_in.approxQuantile(inputCol, self.quantiles, relativeError)\n",
    "        self.quantile_values = [-float('inf')] + quantile_values + [float('inf')]\n",
    "        self.inputCol = inputCol\n",
    "        \n",
    "    def transform(self, df_in):\n",
    "        bucketer = Bucketizer().setSplits(self.quantile_values).setInputCol(self.inputCol)\n",
    "        return bucketer.transform(df_in)\n",
    "        \n",
    "bucketer = UnbalancedQuantileDiscretizer(in_quantiles)\n",
    "bucketer.fit(contDF, \"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|Total|CustomerID|Bucketizer_4834816aa3128350a6b8__output|\n",
      "+-----+----------+---------------------------------------+\n",
      "|  119|   14452.0|                                    0.0|\n",
      "|  440|   16916.0|                                    2.0|\n",
      "|  630|   17633.0|                                    2.0|\n",
      "|   34|   14768.0|                                    0.0|\n",
      "| 1542|   13094.0|                                    3.0|\n",
      "|  854|   17884.0|                                    2.0|\n",
      "|   97|   16596.0|                                    0.0|\n",
      "|  290|   13607.0|                                    1.0|\n",
      "|  541|   14285.0|                                    2.0|\n",
      "|  244|   16561.0|                                    1.0|\n",
      "|  491|   13956.0|                                    2.0|\n",
      "|  204|   13533.0|                                    1.0|\n",
      "|  493|   16629.0|                                    2.0|\n",
      "|  159|   17267.0|                                    0.0|\n",
      "| 1140|   13918.0|                                    3.0|\n",
      "|   55|   18114.0|                                    0.0|\n",
      "|   88|   14473.0|                                    0.0|\n",
      "|  150|   14024.0|                                    0.0|\n",
      "|  206|   12493.0|                                    1.0|\n",
      "|  138|   15776.0|                                    0.0|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basicTransform = SQLTransformer()\\\n",
    "                .setStatement(\"\"\"SELECT sum(Quantity) AS Total, CustomerID FROM __THIS__ GROUP BY CustomerID\"\"\")\n",
    "    \n",
    "in_quantiles = [0.25, 0.50, 0.75, 0.95, 0.99]\n",
    "total_df = basicTransform.transform(sales)\n",
    "\n",
    "bucketer = UnbalancedQuantileDiscretizer(in_quantiles)\n",
    "bucketer.fit(total_df, \"Total\")\n",
    "total_df = bucketer.transform(total_df)\n",
    "total_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110166320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.5, 0.75, 0.95, 0.99]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd_df = total_df.toPandas().sort_values(by='Total').reset_index()\n",
    "buck_col = [x for x in pd_df.columns if \"Bucketizer\" in x][0]\n",
    "for b in pd_df[buck_col].unique():\n",
    "    foo = pd_df.loc[pd_df[buck_col]==b, 'Total'].sort_values()\n",
    "    plt.plot(foo.index, foo.values)\n",
    "plt.show()\n",
    "print(in_quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and normalization\n",
    "\n",
    "### Standard Scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------------------+\n",
      "| id|      features|StandardScaler_4f368cd7bde52dbdff2b__output|\n",
      "+---+--------------+-------------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|                       [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|                       [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|                       [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|                       [3.58568582800318...|\n",
      "+---+--------------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, features=DenseVector([1.0, 0.1, -1.0]), StandardScaler_4f368cd7bde52dbdff2b__output=DenseVector([1.1952, 0.0234, -0.5976]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler(inputCol=\"features\")\n",
    "foo = std_scaler.fit(scaleDF).transform(scaleDF)\n",
    "foo.show()\n",
    "foo.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaler\n",
    "\n",
    "Linearly interpolates between the minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------+\n",
      "| id|      features|MinMaxScaler_42d9b56298a1c71ff511__output|\n",
      "+---+--------------+-----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                            [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                            [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                            [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                            [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                         [10.0,10.0,10.0]|\n",
      "+---+--------------+-----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, features=DenseVector([1.0, 0.1, -1.0]), MinMaxScaler_42d9b56298a1c71ff511__output=DenseVector([5.0, 5.0, 5.0]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "min_max = MinMaxScaler(min=5, max=10, inputCol=\"features\")\n",
    "fitted_min_max = min_max.fit(scaleDF)\n",
    "foo = fitted_min_max.transform(scaleDF)\n",
    "foo.show()\n",
    "foo.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbsScaler\n",
    "Divides by the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------+\n",
      "| id|      features|MaxAbsScaler_4e6589ee2c044cea830e__output|\n",
      "+---+--------------+-----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                     [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|                     [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|                     [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|                     [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                            [1.0,1.0,1.0]|\n",
      "+---+--------------+-----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, features=DenseVector([1.0, 0.1, -1.0]), MaxAbsScaler_4e6589ee2c044cea830e__output=DenseVector([0.3333, 0.0099, -0.3333]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "max_abs = MaxAbsScaler(inputCol=\"features\")\n",
    "fitted_max_abs = max_abs.fit(scaleDF)\n",
    "foo = fitted_max_abs.transform(scaleDF)\n",
    "foo.show()\n",
    "foo.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Product\n",
    "\n",
    "All the input vectors (e.g. features) must have the same length as the scale up vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------+\n",
      "| id|      features|ElementwiseProduct_48079f5e8841b56b979b__output|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                               [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                               [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                               [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                               [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                              [30.0,151.5,60.0]|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, features=DenseVector([1.0, 0.1, -1.0]), ElementwiseProduct_48079f5e8841b56b979b__output=DenseVector([10.0, 1.5, -20.0]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "scale_up_vec = Vectors.dense(10., 15., 20.)\n",
    "scaling_up = ElementwiseProduct(scalingVec=scale_up_vec, inputCol=\"features\")\n",
    "foo = scaling_up.transform(scaleDF)\n",
    "foo.show()\n",
    "foo.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer\n",
    "\n",
    "The parameter `p` drives this normalizer, being p=2 the euclidean norm and p=1 the manhattan norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|Normalizer_4fbb8456ed864621e6e4__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                   [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|                   [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|                   [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|                   [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|                   [0.18633540372670...|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, features=DenseVector([1.0, 0.1, -1.0]), Normalizer_4fbb8456ed864621e6e4__output=DenseVector([0.4762, 0.0476, -0.4762]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "man_distance = Normalizer(p=1, inputCol=\"features\")\n",
    "foo = man_distance.transform(scaleDF)\n",
    "foo.show()\n",
    "foo.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Categorical Features\n",
    "\n",
    "### StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# will transform good to 1. and bad to 0.\n",
    "lbl_indexer = StringIndexer(inputCol=\"lab\", outputCol=\"labelInd\")\n",
    "idx_res = lbl_indexer.fit(simpleDF).transform(simpleDF)\n",
    "idx_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "|  red| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    45| 38.97187133755819|     3.0|\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# also applicable to fields not being Strings, in this case, it will convert to string first\n",
    "# if a value to transform is not fitted, then it will skip it as per directive\n",
    "val_indexer = StringIndexer(inputCol=\"value1\", outputCol=\"valueInd\", handleInvalid=\"skip\")\n",
    "foo = val_indexer.fit(simpleDF).transform(simpleDF)\n",
    "foo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IndexToString\n",
    "\n",
    "When converting string to indexes, Spark keeps the mapping as metadata, when converting that back to strings, it will use this metadata to retrieve the original strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_484aa95f19b4f26c9af5__output|\n",
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                                      good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                                       bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                                       bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                                      good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                                      good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                                       bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                                      good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                                       bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                                       bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|                                       bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|                                      good|\n",
      "|green|good|     1|14.386294994851129|     1.0|                                      good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                                       bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                                       bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                                      good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                                      good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                                       bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                                      good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                                       bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                                       bad|\n",
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "label_reverse = IndexToString(inputCol=\"labelInd\")\n",
    "label_reverse.transform(idx_res).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing in Vectors\n",
    "\n",
    "It will find categorical values inside a vector (e.g. features) and it will allocate an index to just that element in the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|[1.0,2.0,3.0]|    1|\n",
      "|[2.0,5.0,6.0]|    2|\n",
      "|[1.0,8.0,9.0]|    3|\n",
      "+-------------+-----+\n",
      "\n",
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "idx_in = spark.createDataFrame([(Vectors.dense(1, 2, 3), 1), \n",
    "                                (Vectors.dense(2, 5, 6), 2),\n",
    "                                (Vectors.dense(1, 8, 9), 3)]).toDF(\"features\", \"label\")\n",
    "idx_in.show()\n",
    "\n",
    "idxr = VectorIndexer(inputCol=\"features\", outputCol=\"idxed\", maxCategories=2)\n",
    "idxr.fit(idx_in).transform(idx_in).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------------------------------+\n",
      "|color|colorInd|OneHotEncoder_4c5e9739df945b9d6df6__output|\n",
      "+-----+--------+------------------------------------------+\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "+-----+--------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "\n",
    "label_idxr = StringIndexer(inputCol=\"color\", outputCol=\"colorInd\")\n",
    "color_lab = label_idxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder(inputCol=\"colorInd\")\n",
    "ohe.transform(color_lab).show()  # as sparse vectors, obviously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Transformers\n",
    "\n",
    "### Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer(inputCol=\"Description\", outputCol=\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using regular expression\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "rt = RegexTokenizer(inputCol=\"Description\", outputCol=\"DescOut\", pattern=\" \", toLowercase=True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing common words (aka stop words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------------------------------------+\n",
      "|         Description|             DescOut|StopWordsRemover_49b2956a18ba0cee71f4__output|\n",
      "+--------------------+--------------------+---------------------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                         [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                         [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|                         [12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|                         [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|                         [gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|                         [skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|                         [feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|                         [camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|                         [white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|                         [english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|                         [hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|                         [scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|                         [rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|                         [gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|                         [storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|                         [set, 4, knick, k...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|                            [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|                         [grow, flytrap, s...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|                         [airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|                         [airline, bag, vi...|\n",
      "+--------------------+--------------------+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "english_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover(stopWords=english_stop_words, inputCol=\"DescOut\")\n",
    "stops.transform(tokenized).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Ngrams (word combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------------------+\n",
      "|         Description|             DescOut|NGram_42d0a27ded679308db85__output|\n",
      "+--------------------+--------------------+----------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|              [rabbit night, ni...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|              [doughnut lip, li...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|              [12 message, mess...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|              [blue harmonica, ...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|              [gumball coat, co...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|              [skulls ,  water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|              [feltcraft girl, ...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|              [camouflage led, ...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|              [white skull, sku...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|              [english rose, ro...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|              [hot water, water...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|              [scottie dog, dog...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|              [rose caravan, ca...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|              [gingham heart, h...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|              [storage tin, tin...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|              [set of, of 4, 4 ...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|                  [popcorn holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|              [grow a, a flytra...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|              [airline bag, bag...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|              [airline bag, bag...|\n",
      "+--------------------+--------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "bigram = NGram(inputCol=\"DescOut\", n=2)\n",
    "bigram.transform(tokenized).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting words into numerical representations\n",
    "\n",
    "### CountVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|             DescOut|            countVec|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|(500,[150,185,212...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|(500,[462,463,492...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|(500,[35,41,166],...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|(500,[10,16,36,35...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|(500,[228,280,407...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|(500,[11,40,133],...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|(500,[60,64,69],[...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|   (500,[263],[1.0])|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|(500,[15,34,39,40...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|(500,[34,39,40,46...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|(500,[34,39,40,14...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|(500,[34,39,40,14...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|(500,[46,297],[1....|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|(500,[3,4,11,143,...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|(500,[6,45,109,16...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|(500,[0,1,49,70,3...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|(500,[21,296],[1....|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|(500,[36,45,378],...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|(500,[2,6,328],[1...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|(500,[0,2,6,328,4...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Description='RABBIT NIGHT LIGHT', DescOut=['rabbit', 'night', 'light'], countVec=SparseVector(500, {150: 1.0, 185: 1.0, 212: 1.0}))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"DescOut\", outputCol=\"countVec\", vocabSize=500, minTF=1, minDF=2)\n",
    "fitted_cv = cv.fit(tokenized)\n",
    "foo = fitted_cv.transform(tokenized)\n",
    "foo.show()\n",
    "foo.take(1) # output is vocabSize, index of the word and word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                   |IDFOut                                                                                                              |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[155,1152,4291,5981,6756],[1.0,1.0,1.0,1.0,1.0]) |(10000,[155,1152,4291,5981,6756],[0.0,0.0,0.0,0.0,0.0])                                                             |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])         |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                                                                     |\n",
      "|[pin, cushion, babushka, red]          |(10000,[4291,5111,5673,7153],[1.0,1.0,1.0,1.0])         |(10000,[4291,5111,5673,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, mini, cases]          |(10000,[547,1576,2591,4291],[1.0,1.0,1.0,1.0])          |(10000,[547,1576,2591,4291],[0.0,0.0,1.0116009116784799,0.0])                                                       |\n",
      "|[red, kitchen, scales]                 |(10000,[3461,4291,6214],[1.0,1.0,1.0])                  |(10000,[3461,4291,6214],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[large, red, babushka, notebook]       |(10000,[2782,2787,4291,7153],[1.0,1.0,1.0,1.0])         |(10000,[2782,2787,4291,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, oven, glove]          |(10000,[302,2591,4291,8242],[1.0,1.0,1.0,1.0])          |(10000,[302,2591,4291,8242],[0.0,1.0116009116784799,0.0,0.0])                                                       |\n",
      "|[red, retrospot, plate]                |(10000,[2591,4291,4456],[1.0,1.0,1.0])                  |(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0])                                                               |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tf = HashingTF(inputCol=\"DescOut\", outputCol=\"TFOut\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"TFOut\", outputCol=\"IDFOut\", minDocFreq=2)\n",
    "\n",
    "tf_idf_in = tokenized.where(\"array_contains(DescOut, 'red')\").select('DescOut').limit(10)\n",
    "idf_in = tf.transform(tf_idf_in)\n",
    "idf.fit(idf_in).transform(idf_in).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi, I, heard, about, Spark/ Vector: [0.004204302234575153,0.050164113938808444,-0.017830849438905717]\n",
      "Text: I, wish, Java, could, use, case, classes/ Vector: [0.08075640493604754,-0.024336789468569413,-0.01091744750738144]\n",
      "Text: Logistic, regression, models, are, neat/ Vector: [-0.0315695621073246,-0.053468143194913866,-0.0891033224761486]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "documentDF = spark.createDataFrame([(\"Hi I heard about Spark\".split(\" \"), ), \n",
    "                                    (\"I wish Java could use case classes\".split(\" \"), ), \n",
    "                                    (\"Logistic regression models are neat\".split(\" \"), )], [\"text\"])\n",
    "word2vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: {}/ Vector: {}\".format(\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Manipulation\n",
    "\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_46b280937b5b70f26de5__output          |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(inputCol=\"features\", k=2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "pe = PolynomialExpansion(inputCol=\"features\", degree=2)\n",
    "pe.transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "\n",
    "tkn = Tokenizer(inputCol=\"Description\", outputCol=\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\", \"CustomerId\")).where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fitted_cv.transform(tokenized).where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector(featuresCol=\"countVec\", labelCol=\"CustomerId\", numTopFeatures=2)\n",
    "chisq.fit(prechi).transform(prechi).drop(\"CustomerId\", \"Description\", \"DescOut\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pca = pca.fit(scaleDF)\n",
    "fitted_pca.write().overwrite().save(\"../tmp/chapter-25-model\")\n",
    "\n",
    "from pyspark.ml.feature import PCAModel\n",
    "\n",
    "loaded_pca = PCAModel.load(\"../tmp/chapter-25-model\")\n",
    "loaded_pca.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
