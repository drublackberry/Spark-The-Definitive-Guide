{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Data Sources\n",
    "\n",
    "### Read\n",
    "We use the `DataFrameReader` object, available through the `read()` method on `SparkSession`. \n",
    "It will return a DataFrame.\n",
    "It is customizable through options.\n",
    "As a minimum the path of the file to be read must be provided.\n",
    "\n",
    "### Write \n",
    "We use a `DataFrameWriter` available through the `write` method of a DataFrame. It also accepts options.\n",
    "\n",
    "## CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# launch spark with sqlite jar for further or querying\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:' + os.path.abspath('..') + '/jar/sqlite-jdbc-3.25.2.jar pyspark-shell'\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"chapter9\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: specify the path as argument of load()\n",
    "csv_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"failFast\")\\\n",
    "            .load(\"../data/flight-data/csv/2011-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: specify the path as option and leave load() with no arguments\n",
    "csv_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"failFast\")\\\n",
    "            .option(\"path\", \"../data/flight-data/csv/2011-summary.csv\")\\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- NewData: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add a column and store as new file\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "new_df = csv_df.withColumn(\"NewData\", lit(1))\n",
    "new_df.printSchema()\n",
    "\n",
    "# store in a csv file\n",
    "new_df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"../data/custom-data/chapter9-sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df = spark.read.format(\"json\")\\\n",
    "        .option(\"mode\", \"FAILFAST\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"path\", \"../data/flight-data/json/2010-summary.json\")\\\n",
    "        .load()\n",
    "json_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|twice_count|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "|    United States|            Romania|    1|          2|\n",
      "|    United States|            Ireland|  264|        528|\n",
      "|    United States|              India|   69|        138|\n",
      "|            Egypt|      United States|   24|         48|\n",
      "|Equatorial Guinea|      United States|    1|          2|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "new_df = json_df.withColumn(\"twice_count\", (col(\"count\").cast(\"float\") * 2).cast(\"int\"))\n",
    "new_df.show(5)\n",
    "\n",
    "new_df.write.format(\"json\").mode(\"overwrite\").save(\"../data/custom-data/chapter9-sample.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet files\n",
    "\n",
    "The parquet file is compressed (gz). The current flight data from 2010 weights 3.8K in parquet format and 20K in JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_df = spark.read.format(\"parquet\").load(\"../data/flight-data/parquet/2010-summary.parquet\")\n",
    "parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|twice_count|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "|    United States|            Romania|    1|          2|\n",
      "|    United States|            Ireland|  264|        528|\n",
      "|    United States|              India|   69|        138|\n",
      "|            Egypt|      United States|   24|         48|\n",
      "|Equatorial Guinea|      United States|    1|          2|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = parquet_df.withColumn(\"twice_count\", (col(\"count\").cast(\"float\") * 2).cast(\"int\"))\n",
    "new_df.show(5)\n",
    "\n",
    "new_df.write.format(\"parquet\").mode(\"overwrite\").save(\"../data/custom-data/chapter9-sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL databases\n",
    "\n",
    "### SQLite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: decimal(20,0) (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"../data/flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\"\n",
    "\n",
    "dbDF = spark.read.format(\"jdbc\").option(\"url\", url).option('dbtable', tablename).option(\"driver\", driver).load()\n",
    "dbDF.printSchema()\n",
    "dbDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query pushdown\n",
    "Instead of a specifying a full table, specify a query and load the results in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Egypt|      United States|   24|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#224,ORIGIN_COUNTRY_NAME#225,count#226] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>\n"
     ]
    }
   ],
   "source": [
    "dbDF.filter(col(\"DEST_COUNTRY_NAME\")==\"Egypt\").show()\n",
    "dbDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Egypt|      United States|   24|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation((SELECT * FROM flight_info WHERE DEST_COUNTRY_NAME='Egypt') AS flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#253,ORIGIN_COUNTRY_NAME#254,count#255] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>\n"
     ]
    }
   ],
   "source": [
    "pushdown_query = \"\"\"(SELECT * FROM flight_info WHERE DEST_COUNTRY_NAME='Egypt') AS flight_info\"\"\"\n",
    "pdDF = spark.read.format(\"jdbc\").option(\"url\", url).option('dbtable', pushdown_query).option(\"driver\", driver).load()\n",
    "pdDF.show()\n",
    "pdDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from databases in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDF = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    "                                .option('dbtable', tablename)\\\n",
    "                                .option(\"driver\", driver)\\\n",
    "                                .option(\"numPartitions\", 10)\\\n",
    "                                .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Sweden|      United States|   65|\n",
      "|    United States|             Sweden|   73|\n",
      "|         Anguilla|      United States|   21|\n",
      "|    United States|           Anguilla|   20|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specifying predicates, that would create two partitions one for each predicate\n",
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "              \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the predicates are not disjoint, we get lots of rows\n",
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n",
    "              \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    348113|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "dbDF.select(max(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName = \"count\"\n",
    "lowerBound = 0\n",
    "upperBound = 348113\n",
    "numPartitions = 10\n",
    "\n",
    "spark.read.jdbc(url, tablename, column=colName, \n",
    "                lowerBound=lowerBound, upperBound=upperBound, \n",
    "                numPartitions=numPartitions, \n",
    "                properties=props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_url = url = \"jdbc:sqlite:\" + os.path.abspath('..') + '/data/custom-data/chapter9-sample.db'\n",
    "csv_df.write.jdbc(new_url, tablename, mode=\"overwrite\", properties=props)\n",
    "# inspect the results\n",
    "spark.read.jdbc(new_url, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appending to the new db\n",
    "csv_df.write.jdbc(new_url, tablename, mode=\"append\", properties=props)\n",
    "spark.read.jdbc(new_url, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, S...|\n",
      "|[United States, G...|\n",
      "|[United States, C...|\n",
      "|[United States, R...|\n",
      "|[United States, I...|\n",
      "|[Egypt, United St...|\n",
      "|[United States, I...|\n",
      "|[United States, S...|\n",
      "|[United States, G...|\n",
      "|[Costa Rica, Unit...|\n",
      "|[Senegal, United ...|\n",
      "|[Guyana, United S...|\n",
      "|[United States, M...|\n",
      "|[United States, S...|\n",
      "|[Malta, United St...|\n",
      "|[Bolivia, United ...|\n",
      "|[Anguilla, United...|\n",
      "|[United States, P...|\n",
      "|[United States, G...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(\"../data/flight-data/csv/2011-summary.csv\").selectExpr(\"split(value, ',') as rows\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.select(\"DEST_COUNTRY_NAME\").write.text(\"../data/custom-data/chapter9-sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by performing partitioning you can write more columns, as directories\n",
    "csv_df.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\").write.partitionBy(\"count\")\\\n",
    "                .text(\"../data/custom-data/chapter9-sample-partitioned.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
