{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"chapter7\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\").load(\"../data/retail-data/all/*.csv\")\\\n",
    ".coalesce(5)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is an action\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# this is a transformation\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get an approximation (less accurate but faster)\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(InvoiceNo='536365', StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, InvoiceDate='12/1/2010 8:26', UnitPrice=2.55, CustomerID=17850, Country='United Kingdom')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is an action\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   avg(Quantity)|\n",
      "+----------------+\n",
      "|9.55224954743324|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660879| 47559.39140929848|  218.08095663447733|   218.08115785023355|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, var_samp, stddev_pop, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|skewness(Quantity)|kurtosis(Quantity)|\n",
      "+------------------+------------------+\n",
      "|-0.264075576105298|119768.05495534067|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085636837E-4|             1052.7280543912716|            1052.7260778751674|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import covar_pop, covar_samp, corr\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "# returns a set and a list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x110e60588>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"InvoiceNo\", \"CustomerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, expr\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\").alias(\"quan\"), expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2010-12-01|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.select(\"date\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x110e992b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\").\\\n",
    "orderBy(desc(\"Quantity\")).\\\n",
    "rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "windowSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxPurchaseQuantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----------+----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|qualityRank|qualityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+-----------+----------------+-------------------+\n",
      "|     12477|2011-04-14|     100|          1|               1|                100|\n",
      "|     12477|2011-04-14|      72|          2|               2|                100|\n",
      "|     12477|2011-04-14|      36|          3|               3|                100|\n",
      "|     12477|2011-04-14|      36|          3|               3|                100|\n",
      "|     12477|2011-04-14|      36|          3|               3|                100|\n",
      "|     12477|2011-04-14|      24|          6|               4|                100|\n",
      "|     12477|2011-04-14|      24|          6|               4|                100|\n",
      "|     12477|2011-04-14|      24|          6|               4|                100|\n",
      "|     12477|2011-04-14|      20|          9|               5|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "|     12477|2011-04-14|      12|         10|               6|                100|\n",
      "+----------+----------+--------+-----------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.select(col(\"CustomerId\"), col(\"date\"), col(\"Quantity\"),\n",
    "                  purchaseRank.alias(\"qualityRank\"), \n",
    "                  purchaseDenseRank.alias(\"qualityDenseRank\"),\n",
    "                  maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|      Date|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|      null|          null|      5176450|\n",
      "|2010-12-01|          null|        26814|\n",
      "|2010-12-01|     Australia|          107|\n",
      "|2010-12-01|        France|          449|\n",
      "|2010-12-01|United Kingdom|        23949|\n",
      "|2010-12-01|          EIRE|          243|\n",
      "|2010-12-01|        Norway|         1852|\n",
      "|2010-12-01|   Netherlands|           97|\n",
      "|2010-12-01|       Germany|          117|\n",
      "|2010-12-02|       Germany|          146|\n",
      "|2010-12-02|          EIRE|            4|\n",
      "|2010-12-02|          null|        21023|\n",
      "|2010-12-02|United Kingdom|        20873|\n",
      "|2010-12-03|        Poland|          140|\n",
      "|2010-12-03|        France|          239|\n",
      "|2010-12-03|       Germany|          170|\n",
      "|2010-12-03|      Portugal|           65|\n",
      "|2010-12-03|         Spain|          400|\n",
      "|2010-12-03|   Switzerland|          110|\n",
      "|2010-12-03|       Belgium|          528|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNotNull = dfWithDate.drop()\n",
    "rolledUpDf = dfNotNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\")).orderBy(\"Date\")\n",
    "rolledUpDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+------------------+---------------+\n",
      "|      Date|       Country|sum(Quantity)|    sum(UnitPrice)|sum(CustomerID)|\n",
      "+----------+--------------+-------------+------------------+---------------+\n",
      "|2011-02-14|          EIRE|         1502|            339.69|        1075856|\n",
      "|2011-02-23|        France|          362|             49.33|         261177|\n",
      "|2011-04-04|         Japan|         -624|              2.55|          12755|\n",
      "|2011-06-05|       Germany|          529|177.08999999999997|         716690|\n",
      "|2011-07-24|     Australia|          214| 57.56999999999999|         248620|\n",
      "|2011-10-18|          EIRE|          625|193.95999999999998|         743096|\n",
      "|2010-12-03|        France|          239| 89.88000000000002|         291714|\n",
      "|2011-06-09|        Sweden|          188|38.629999999999995|         137313|\n",
      "|2011-06-12|       Germany|          345|             37.21|         162708|\n",
      "|2011-06-22|        Norway|            1|             700.0|          12432|\n",
      "|2011-09-30|United Kingdom|        17552| 6476.789999999998|       20664710|\n",
      "|2011-04-04|          EIRE|         1239|141.14000000000001|         421849|\n",
      "|2011-04-08|      Portugal|          370|50.989999999999995|         259476|\n",
      "|2011-07-25|        Poland|           -3|              2.95|          12576|\n",
      "|2011-08-15|         Malta|          303|191.86999999999995|         696600|\n",
      "|2011-10-27|   Netherlands|         5769|49.230000000000004|         410088|\n",
      "|2011-11-29|       Germany|          639|172.52999999999997|         653931|\n",
      "|2011-02-06|United Kingdom|         2048| 799.4500000000007|        4357107|\n",
      "|2011-03-14|       Belgium|           91|             83.29|         185745|\n",
      "|2011-04-07|United Kingdom|         8348| 4232.259999999991|       17922030|\n",
      "+----------+--------------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNotNull.groupBy(\"Date\", \"Country\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
